{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuguochao/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, concatenate, regularizers, \\\n",
    "                          Conv2D, MaxPool2D, Flatten, Activation, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "import utils\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(input_shape):\n",
    "    model_input = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(512, kernel_size=(1, 7), padding='same')(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "#     x = MaxPool2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(256, kernel_size=(1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "#     x = MaxPool2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(128, kernel_size=(1, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "#     x = MaxPool2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Conv2D(64, kernel_size=(1, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "#     x = MaxPool2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(32, kernel_size=(1, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "#     x = MaxPool2D(pool_size=(1, 4))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "    \n",
    "    model_output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "y = data.pop('label').values\n",
    "X = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = 10\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "opt = Adam(lr=0.0001)\n",
    "class_weight = {1:20, 0:1}\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "random_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.1463 train_acc: 0.9801, train_auc: 0.7787 train_ks: 0.4383, val_auc: 0.7646 val_ks: 0.4180, test_auc: 0.7909 test_ks: 0.4663\n",
      "Epoch 2 train_loss: 0.1432 train_acc: 0.9758, train_auc: 0.7908 train_ks: 0.4575, val_auc: 0.7804 val_ks: 0.4364, test_auc: 0.8029 test_ks: 0.4826\n",
      "Epoch 3 train_loss: 0.1335 train_acc: 0.9800, train_auc: 0.7986 train_ks: 0.4649, val_auc: 0.7800 val_ks: 0.4482, test_auc: 0.7974 test_ks: 0.4660\n",
      "Epoch 4 train_loss: 0.1422 train_acc: 0.9756, train_auc: 0.8018 train_ks: 0.4739, val_auc: 0.7792 val_ks: 0.4430, test_auc: 0.7946 test_ks: 0.4515\n",
      "Epoch 5 train_loss: 0.1384 train_acc: 0.9780, train_auc: 0.8041 train_ks: 0.4694, val_auc: 0.7808 val_ks: 0.4451, test_auc: 0.7906 test_ks: 0.4571\n",
      "Epoch 6 train_loss: 0.1450 train_acc: 0.9732, train_auc: 0.8087 train_ks: 0.4793, val_auc: 0.7828 val_ks: 0.4525, test_auc: 0.7926 test_ks: 0.4567\n",
      "Epoch 7 train_loss: 0.1468 train_acc: 0.9740, train_auc: 0.8088 train_ks: 0.4769, val_auc: 0.7854 val_ks: 0.4586, test_auc: 0.7896 test_ks: 0.4713\n",
      "Epoch 8 train_loss: 0.1558 train_acc: 0.9687, train_auc: 0.8135 train_ks: 0.4820, val_auc: 0.7879 val_ks: 0.4631, test_auc: 0.7906 test_ks: 0.4891\n",
      "Epoch 9 train_loss: 0.1419 train_acc: 0.9733, train_auc: 0.8124 train_ks: 0.4825, val_auc: 0.7855 val_ks: 0.4672, test_auc: 0.7880 test_ks: 0.4764\n",
      "Epoch 10 train_loss: 0.1500 train_acc: 0.9697, train_auc: 0.8138 train_ks: 0.4846, val_auc: 0.7850 val_ks: 0.4779, test_auc: 0.7856 test_ks: 0.4606\n",
      "Epoch 11 train_loss: 0.1455 train_acc: 0.9713, train_auc: 0.8165 train_ks: 0.4879, val_auc: 0.7824 val_ks: 0.4622, test_auc: 0.7895 test_ks: 0.4585\n",
      "Epoch 12 train_loss: 0.1585 train_acc: 0.9653, train_auc: 0.8192 train_ks: 0.4863, val_auc: 0.7852 val_ks: 0.4871, test_auc: 0.7851 test_ks: 0.4590\n",
      "Epoch 13 train_loss: 0.1567 train_acc: 0.9694, train_auc: 0.8214 train_ks: 0.4927, val_auc: 0.7860 val_ks: 0.4847, test_auc: 0.7830 test_ks: 0.4724\n",
      "Epoch 14 train_loss: 0.1613 train_acc: 0.9674, train_auc: 0.8224 train_ks: 0.4984, val_auc: 0.7880 val_ks: 0.4716, test_auc: 0.7858 test_ks: 0.4680\n",
      "Epoch 15 train_loss: 0.1575 train_acc: 0.9676, train_auc: 0.8242 train_ks: 0.5048, val_auc: 0.7867 val_ks: 0.4691, test_auc: 0.7846 test_ks: 0.4852\n",
      "Epoch 16 train_loss: 0.1575 train_acc: 0.9688, train_auc: 0.8264 train_ks: 0.5021, val_auc: 0.7815 val_ks: 0.4579, test_auc: 0.7854 test_ks: 0.4621\n",
      "Epoch 17 train_loss: 0.1485 train_acc: 0.9703, train_auc: 0.8278 train_ks: 0.5094, val_auc: 0.7821 val_ks: 0.4571, test_auc: 0.7817 test_ks: 0.4556\n",
      "Epoch 18 train_loss: 0.1426 train_acc: 0.9731, train_auc: 0.8295 train_ks: 0.5079, val_auc: 0.7838 val_ks: 0.4786, test_auc: 0.7823 test_ks: 0.4627\n",
      "Epoch 19 train_loss: 0.1253 train_acc: 0.9772, train_auc: 0.8303 train_ks: 0.5093, val_auc: 0.7852 val_ks: 0.4774, test_auc: 0.7807 test_ks: 0.4450\n",
      "Epoch 20 train_loss: 0.1281 train_acc: 0.9766, train_auc: 0.8310 train_ks: 0.5096, val_auc: 0.7748 val_ks: 0.4421, test_auc: 0.7739 test_ks: 0.4413\n",
      "Epoch 21 train_loss: 0.1558 train_acc: 0.9698, train_auc: 0.8350 train_ks: 0.5147, val_auc: 0.7764 val_ks: 0.4556, test_auc: 0.7753 test_ks: 0.4247\n",
      "Epoch 22 train_loss: 0.1387 train_acc: 0.9740, train_auc: 0.8363 train_ks: 0.5172, val_auc: 0.7765 val_ks: 0.4698, test_auc: 0.7700 test_ks: 0.4245\n",
      "Epoch 23 train_loss: 0.1407 train_acc: 0.9728, train_auc: 0.8375 train_ks: 0.5263, val_auc: 0.7734 val_ks: 0.4442, test_auc: 0.7709 test_ks: 0.4304\n",
      "Epoch 24 train_loss: 0.1422 train_acc: 0.9708, train_auc: 0.8393 train_ks: 0.5225, val_auc: 0.7767 val_ks: 0.4698, test_auc: 0.7713 test_ks: 0.4563\n",
      "Epoch 25 train_loss: 0.1571 train_acc: 0.9676, train_auc: 0.8404 train_ks: 0.5193, val_auc: 0.7712 val_ks: 0.4428, test_auc: 0.7721 test_ks: 0.4339\n",
      "Epoch 26 train_loss: 0.1412 train_acc: 0.9716, train_auc: 0.8416 train_ks: 0.5266, val_auc: 0.7709 val_ks: 0.4505, test_auc: 0.7689 test_ks: 0.4489\n",
      "Epoch 27 train_loss: 0.1732 train_acc: 0.9623, train_auc: 0.8447 train_ks: 0.5317, val_auc: 0.7656 val_ks: 0.4362, test_auc: 0.7681 test_ks: 0.4357\n",
      "Epoch 28 train_loss: 0.1590 train_acc: 0.9642, train_auc: 0.8436 train_ks: 0.5232, val_auc: 0.7697 val_ks: 0.4407, test_auc: 0.7653 test_ks: 0.4301\n",
      "Epoch 29 train_loss: 0.1721 train_acc: 0.9618, train_auc: 0.8474 train_ks: 0.5367, val_auc: 0.7721 val_ks: 0.4627, test_auc: 0.7688 test_ks: 0.4427\n",
      "Epoch 30 train_loss: 0.1803 train_acc: 0.9595, train_auc: 0.8482 train_ks: 0.5338, val_auc: 0.7645 val_ks: 0.4360, test_auc: 0.7661 test_ks: 0.4225\n",
      "\n",
      "2 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.1309 train_acc: 0.9842, train_auc: 0.7728 train_ks: 0.4303, val_auc: 0.7835 val_ks: 0.4719, test_auc: 0.7763 test_ks: 0.4383\n",
      "Epoch 2 train_loss: 0.1388 train_acc: 0.9789, train_auc: 0.7894 train_ks: 0.4577, val_auc: 0.7906 val_ks: 0.4799, test_auc: 0.7810 test_ks: 0.4413\n",
      "Epoch 3 train_loss: 0.1127 train_acc: 0.9796, train_auc: 0.7931 train_ks: 0.4546, val_auc: 0.7884 val_ks: 0.4754, test_auc: 0.7854 test_ks: 0.4752\n",
      "Epoch 4 train_loss: 0.1225 train_acc: 0.9794, train_auc: 0.7971 train_ks: 0.4638, val_auc: 0.7934 val_ks: 0.4720, test_auc: 0.7881 test_ks: 0.4654\n",
      "Epoch 5 train_loss: 0.1375 train_acc: 0.9753, train_auc: 0.8015 train_ks: 0.4662, val_auc: 0.7947 val_ks: 0.4781, test_auc: 0.7865 test_ks: 0.4692\n",
      "Epoch 6 train_loss: 0.1456 train_acc: 0.9701, train_auc: 0.8038 train_ks: 0.4686, val_auc: 0.7944 val_ks: 0.5014, test_auc: 0.7880 test_ks: 0.4643\n",
      "Epoch 7 train_loss: 0.1586 train_acc: 0.9690, train_auc: 0.8075 train_ks: 0.4810, val_auc: 0.7942 val_ks: 0.4720, test_auc: 0.7870 test_ks: 0.4716\n",
      "Epoch 8 train_loss: 0.1550 train_acc: 0.9685, train_auc: 0.8093 train_ks: 0.4838, val_auc: 0.7913 val_ks: 0.4729, test_auc: 0.7878 test_ks: 0.4714\n",
      "Epoch 9 train_loss: 0.1551 train_acc: 0.9675, train_auc: 0.8120 train_ks: 0.4909, val_auc: 0.7909 val_ks: 0.4893, test_auc: 0.7914 test_ks: 0.4684\n",
      "Epoch 10 train_loss: 0.1593 train_acc: 0.9640, train_auc: 0.8141 train_ks: 0.4834, val_auc: 0.7915 val_ks: 0.4735, test_auc: 0.7897 test_ks: 0.4710\n",
      "Epoch 11 train_loss: 0.1384 train_acc: 0.9679, train_auc: 0.8120 train_ks: 0.4857, val_auc: 0.7893 val_ks: 0.4630, test_auc: 0.7926 test_ks: 0.4798\n",
      "Epoch 12 train_loss: 0.1563 train_acc: 0.9671, train_auc: 0.8170 train_ks: 0.4906, val_auc: 0.7873 val_ks: 0.4671, test_auc: 0.7906 test_ks: 0.4819\n",
      "Epoch 13 train_loss: 0.1607 train_acc: 0.9622, train_auc: 0.8177 train_ks: 0.4953, val_auc: 0.7831 val_ks: 0.4645, test_auc: 0.7910 test_ks: 0.4785\n",
      "Epoch 14 train_loss: 0.1822 train_acc: 0.9575, train_auc: 0.8214 train_ks: 0.4963, val_auc: 0.7846 val_ks: 0.4819, test_auc: 0.7907 test_ks: 0.4918\n",
      "Epoch 15 train_loss: 0.1841 train_acc: 0.9551, train_auc: 0.8219 train_ks: 0.4952, val_auc: 0.7827 val_ks: 0.4624, test_auc: 0.7886 test_ks: 0.4725\n",
      "Epoch 16 train_loss: 0.1725 train_acc: 0.9582, train_auc: 0.8230 train_ks: 0.5021, val_auc: 0.7834 val_ks: 0.4642, test_auc: 0.7903 test_ks: 0.4720\n",
      "Epoch 17 train_loss: 0.1927 train_acc: 0.9517, train_auc: 0.8235 train_ks: 0.4991, val_auc: 0.7836 val_ks: 0.4662, test_auc: 0.7880 test_ks: 0.4723\n",
      "Epoch 18 train_loss: 0.1595 train_acc: 0.9613, train_auc: 0.8245 train_ks: 0.5012, val_auc: 0.7811 val_ks: 0.4644, test_auc: 0.7940 test_ks: 0.4976\n",
      "Epoch 19 train_loss: 0.1668 train_acc: 0.9568, train_auc: 0.8253 train_ks: 0.5021, val_auc: 0.7775 val_ks: 0.4675, test_auc: 0.7884 test_ks: 0.4721\n",
      "Epoch 20 train_loss: 0.1694 train_acc: 0.9578, train_auc: 0.8265 train_ks: 0.5024, val_auc: 0.7794 val_ks: 0.4660, test_auc: 0.7914 test_ks: 0.4794\n",
      "Epoch 21 train_loss: 0.1856 train_acc: 0.9521, train_auc: 0.8278 train_ks: 0.5115, val_auc: 0.7746 val_ks: 0.4592, test_auc: 0.7937 test_ks: 0.4939\n",
      "Epoch 22 train_loss: 0.1718 train_acc: 0.9578, train_auc: 0.8292 train_ks: 0.5107, val_auc: 0.7812 val_ks: 0.4868, test_auc: 0.7912 test_ks: 0.4975\n",
      "Epoch 23 train_loss: 0.1777 train_acc: 0.9547, train_auc: 0.8289 train_ks: 0.5149, val_auc: 0.7782 val_ks: 0.4693, test_auc: 0.7917 test_ks: 0.4902\n",
      "Epoch 24 train_loss: 0.1687 train_acc: 0.9576, train_auc: 0.8298 train_ks: 0.5173, val_auc: 0.7784 val_ks: 0.4565, test_auc: 0.7883 test_ks: 0.4965\n",
      "Epoch 25 train_loss: 0.1746 train_acc: 0.9560, train_auc: 0.8304 train_ks: 0.5193, val_auc: 0.7744 val_ks: 0.4504, test_auc: 0.7900 test_ks: 0.4896\n",
      "Epoch 26 train_loss: 0.1700 train_acc: 0.9581, train_auc: 0.8316 train_ks: 0.5225, val_auc: 0.7676 val_ks: 0.4239, test_auc: 0.7868 test_ks: 0.4919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 train_loss: 0.1636 train_acc: 0.9581, train_auc: 0.8338 train_ks: 0.5199, val_auc: 0.7688 val_ks: 0.4227, test_auc: 0.7889 test_ks: 0.4850\n",
      "Epoch 28 train_loss: 0.1601 train_acc: 0.9599, train_auc: 0.8359 train_ks: 0.5253, val_auc: 0.7700 val_ks: 0.4237, test_auc: 0.7868 test_ks: 0.4818\n",
      "Epoch 29 train_loss: 0.1583 train_acc: 0.9593, train_auc: 0.8339 train_ks: 0.5250, val_auc: 0.7710 val_ks: 0.4266, test_auc: 0.7887 test_ks: 0.4839\n",
      "Epoch 30 train_loss: 0.1510 train_acc: 0.9630, train_auc: 0.8374 train_ks: 0.5316, val_auc: 0.7703 val_ks: 0.4386, test_auc: 0.7820 test_ks: 0.4821\n",
      "\n",
      "3 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.2974 train_acc: 0.9285, train_auc: 0.7751 train_ks: 0.4229, val_auc: 0.7922 val_ks: 0.4783, test_auc: 0.7620 test_ks: 0.4080\n",
      "Epoch 2 train_loss: 0.2537 train_acc: 0.9400, train_auc: 0.7876 train_ks: 0.4435, val_auc: 0.7872 val_ks: 0.4427, test_auc: 0.7667 test_ks: 0.4294\n",
      "Epoch 3 train_loss: 0.2820 train_acc: 0.9263, train_auc: 0.7990 train_ks: 0.4583, val_auc: 0.7983 val_ks: 0.4608, test_auc: 0.7720 test_ks: 0.4291\n",
      "Epoch 4 train_loss: 0.2800 train_acc: 0.9214, train_auc: 0.8025 train_ks: 0.4663, val_auc: 0.7972 val_ks: 0.4628, test_auc: 0.7708 test_ks: 0.4248\n",
      "Epoch 5 train_loss: 0.2338 train_acc: 0.9410, train_auc: 0.8048 train_ks: 0.4734, val_auc: 0.7948 val_ks: 0.4739, test_auc: 0.7721 test_ks: 0.4182\n",
      "Epoch 6 train_loss: 0.2441 train_acc: 0.9406, train_auc: 0.8079 train_ks: 0.4746, val_auc: 0.7969 val_ks: 0.4637, test_auc: 0.7706 test_ks: 0.4012\n",
      "Epoch 7 train_loss: 0.2342 train_acc: 0.9437, train_auc: 0.8109 train_ks: 0.4768, val_auc: 0.7917 val_ks: 0.4465, test_auc: 0.7769 test_ks: 0.4251\n",
      "Epoch 8 train_loss: 0.2340 train_acc: 0.9414, train_auc: 0.8139 train_ks: 0.4867, val_auc: 0.7866 val_ks: 0.4284, test_auc: 0.7748 test_ks: 0.4111\n",
      "Epoch 9 train_loss: 0.2208 train_acc: 0.9473, train_auc: 0.8166 train_ks: 0.4999, val_auc: 0.7881 val_ks: 0.4322, test_auc: 0.7710 test_ks: 0.4058\n",
      "Epoch 10 train_loss: 0.2174 train_acc: 0.9498, train_auc: 0.8156 train_ks: 0.4942, val_auc: 0.7804 val_ks: 0.4284, test_auc: 0.7720 test_ks: 0.4238\n",
      "Epoch 11 train_loss: 0.2065 train_acc: 0.9536, train_auc: 0.8221 train_ks: 0.5071, val_auc: 0.7826 val_ks: 0.4273, test_auc: 0.7793 test_ks: 0.4156\n",
      "Epoch 12 train_loss: 0.2108 train_acc: 0.9475, train_auc: 0.8233 train_ks: 0.5034, val_auc: 0.7829 val_ks: 0.4124, test_auc: 0.7812 test_ks: 0.4293\n",
      "Epoch 13 train_loss: 0.1977 train_acc: 0.9566, train_auc: 0.8254 train_ks: 0.5098, val_auc: 0.7788 val_ks: 0.4150, test_auc: 0.7775 test_ks: 0.4206\n",
      "Epoch 14 train_loss: 0.2063 train_acc: 0.9533, train_auc: 0.8251 train_ks: 0.5070, val_auc: 0.7798 val_ks: 0.4187, test_auc: 0.7797 test_ks: 0.4244\n",
      "Epoch 15 train_loss: 0.1788 train_acc: 0.9636, train_auc: 0.8275 train_ks: 0.5102, val_auc: 0.7741 val_ks: 0.4058, test_auc: 0.7796 test_ks: 0.4291\n",
      "Epoch 16 train_loss: 0.1808 train_acc: 0.9628, train_auc: 0.8300 train_ks: 0.5129, val_auc: 0.7772 val_ks: 0.4385, test_auc: 0.7807 test_ks: 0.4275\n",
      "Epoch 17 train_loss: 0.1641 train_acc: 0.9676, train_auc: 0.8299 train_ks: 0.5114, val_auc: 0.7739 val_ks: 0.4229, test_auc: 0.7790 test_ks: 0.4489\n",
      "Epoch 18 train_loss: 0.1485 train_acc: 0.9722, train_auc: 0.8318 train_ks: 0.5129, val_auc: 0.7690 val_ks: 0.4138, test_auc: 0.7784 test_ks: 0.4526\n",
      "Epoch 19 train_loss: 0.1531 train_acc: 0.9705, train_auc: 0.8331 train_ks: 0.5189, val_auc: 0.7667 val_ks: 0.4193, test_auc: 0.7778 test_ks: 0.4323\n",
      "Epoch 20 train_loss: 0.1455 train_acc: 0.9722, train_auc: 0.8338 train_ks: 0.5207, val_auc: 0.7697 val_ks: 0.4063, test_auc: 0.7785 test_ks: 0.4402\n",
      "Epoch 21 train_loss: 0.1464 train_acc: 0.9728, train_auc: 0.8345 train_ks: 0.5200, val_auc: 0.7707 val_ks: 0.4325, test_auc: 0.7799 test_ks: 0.4362\n",
      "Epoch 22 train_loss: 0.1537 train_acc: 0.9696, train_auc: 0.8345 train_ks: 0.5188, val_auc: 0.7735 val_ks: 0.4352, test_auc: 0.7828 test_ks: 0.4415\n",
      "Epoch 23 train_loss: 0.1530 train_acc: 0.9685, train_auc: 0.8368 train_ks: 0.5189, val_auc: 0.7730 val_ks: 0.4497, test_auc: 0.7851 test_ks: 0.4350\n",
      "Epoch 24 train_loss: 0.1560 train_acc: 0.9693, train_auc: 0.8393 train_ks: 0.5277, val_auc: 0.7639 val_ks: 0.4216, test_auc: 0.7839 test_ks: 0.4351\n",
      "Epoch 25 train_loss: 0.1579 train_acc: 0.9684, train_auc: 0.8406 train_ks: 0.5296, val_auc: 0.7647 val_ks: 0.4163, test_auc: 0.7815 test_ks: 0.4457\n",
      "Epoch 26 train_loss: 0.1615 train_acc: 0.9677, train_auc: 0.8399 train_ks: 0.5284, val_auc: 0.7585 val_ks: 0.4213, test_auc: 0.7794 test_ks: 0.4313\n",
      "Epoch 27 train_loss: 0.1586 train_acc: 0.9685, train_auc: 0.8442 train_ks: 0.5336, val_auc: 0.7620 val_ks: 0.4108, test_auc: 0.7816 test_ks: 0.4237\n",
      "Epoch 28 train_loss: 0.1720 train_acc: 0.9623, train_auc: 0.8422 train_ks: 0.5292, val_auc: 0.7512 val_ks: 0.4018, test_auc: 0.7820 test_ks: 0.4548\n",
      "Epoch 29 train_loss: 0.1637 train_acc: 0.9644, train_auc: 0.8453 train_ks: 0.5392, val_auc: 0.7573 val_ks: 0.4135, test_auc: 0.7842 test_ks: 0.4451\n",
      "Epoch 30 train_loss: 0.1857 train_acc: 0.9562, train_auc: 0.8439 train_ks: 0.5290, val_auc: 0.7522 val_ks: 0.3892, test_auc: 0.7775 test_ks: 0.4562\n",
      "\n",
      "4 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.0958 train_acc: 0.9885, train_auc: 0.7799 train_ks: 0.4281, val_auc: 0.7601 val_ks: 0.4453, test_auc: 0.7143 test_ks: 0.3418\n",
      "Epoch 2 train_loss: 0.0889 train_acc: 0.9888, train_auc: 0.7944 train_ks: 0.4540, val_auc: 0.7618 val_ks: 0.4546, test_auc: 0.7394 test_ks: 0.4427\n",
      "Epoch 3 train_loss: 0.0992 train_acc: 0.9866, train_auc: 0.7963 train_ks: 0.4447, val_auc: 0.7749 val_ks: 0.4874, test_auc: 0.7212 test_ks: 0.3619\n",
      "Epoch 4 train_loss: 0.1065 train_acc: 0.9875, train_auc: 0.8053 train_ks: 0.4733, val_auc: 0.7719 val_ks: 0.4483, test_auc: 0.7377 test_ks: 0.4176\n",
      "Epoch 5 train_loss: 0.1097 train_acc: 0.9842, train_auc: 0.8094 train_ks: 0.4786, val_auc: 0.7697 val_ks: 0.4548, test_auc: 0.7455 test_ks: 0.4390\n",
      "Epoch 6 train_loss: 0.1226 train_acc: 0.9812, train_auc: 0.8128 train_ks: 0.4801, val_auc: 0.7685 val_ks: 0.4617, test_auc: 0.7395 test_ks: 0.4525\n",
      "Epoch 7 train_loss: 0.1210 train_acc: 0.9810, train_auc: 0.8128 train_ks: 0.4794, val_auc: 0.7695 val_ks: 0.4488, test_auc: 0.7401 test_ks: 0.4334\n",
      "Epoch 8 train_loss: 0.1300 train_acc: 0.9792, train_auc: 0.8137 train_ks: 0.4778, val_auc: 0.7638 val_ks: 0.4623, test_auc: 0.7432 test_ks: 0.4645\n",
      "Epoch 9 train_loss: 0.1180 train_acc: 0.9825, train_auc: 0.8169 train_ks: 0.4838, val_auc: 0.7749 val_ks: 0.4515, test_auc: 0.7365 test_ks: 0.4298\n",
      "Epoch 10 train_loss: 0.1242 train_acc: 0.9816, train_auc: 0.8185 train_ks: 0.4847, val_auc: 0.7708 val_ks: 0.4518, test_auc: 0.7381 test_ks: 0.4329\n",
      "Epoch 11 train_loss: 0.1409 train_acc: 0.9767, train_auc: 0.8198 train_ks: 0.4841, val_auc: 0.7707 val_ks: 0.4452, test_auc: 0.7354 test_ks: 0.4399\n",
      "Epoch 12 train_loss: 0.1542 train_acc: 0.9725, train_auc: 0.8199 train_ks: 0.4876, val_auc: 0.7650 val_ks: 0.4412, test_auc: 0.7341 test_ks: 0.4400\n",
      "Epoch 13 train_loss: 0.1536 train_acc: 0.9713, train_auc: 0.8226 train_ks: 0.4920, val_auc: 0.7691 val_ks: 0.4469, test_auc: 0.7304 test_ks: 0.4170\n",
      "Epoch 14 train_loss: 0.1655 train_acc: 0.9688, train_auc: 0.8221 train_ks: 0.4948, val_auc: 0.7643 val_ks: 0.4546, test_auc: 0.7350 test_ks: 0.4252\n",
      "Epoch 15 train_loss: 0.1761 train_acc: 0.9652, train_auc: 0.8228 train_ks: 0.4958, val_auc: 0.7636 val_ks: 0.4563, test_auc: 0.7369 test_ks: 0.4231\n",
      "Epoch 16 train_loss: 0.1894 train_acc: 0.9599, train_auc: 0.8264 train_ks: 0.5049, val_auc: 0.7668 val_ks: 0.4656, test_auc: 0.7361 test_ks: 0.4154\n",
      "Epoch 17 train_loss: 0.1977 train_acc: 0.9577, train_auc: 0.8272 train_ks: 0.4976, val_auc: 0.7653 val_ks: 0.4485, test_auc: 0.7316 test_ks: 0.4174\n",
      "Epoch 18 train_loss: 0.1903 train_acc: 0.9597, train_auc: 0.8280 train_ks: 0.4961, val_auc: 0.7653 val_ks: 0.4411, test_auc: 0.7336 test_ks: 0.4186\n",
      "Epoch 19 train_loss: 0.2038 train_acc: 0.9530, train_auc: 0.8281 train_ks: 0.4966, val_auc: 0.7600 val_ks: 0.4350, test_auc: 0.7332 test_ks: 0.4236\n",
      "Epoch 20 train_loss: 0.2077 train_acc: 0.9536, train_auc: 0.8296 train_ks: 0.5052, val_auc: 0.7575 val_ks: 0.4368, test_auc: 0.7331 test_ks: 0.4203\n",
      "Epoch 21 train_loss: 0.2005 train_acc: 0.9541, train_auc: 0.8326 train_ks: 0.5072, val_auc: 0.7588 val_ks: 0.4487, test_auc: 0.7306 test_ks: 0.4183\n",
      "Epoch 22 train_loss: 0.2174 train_acc: 0.9463, train_auc: 0.8331 train_ks: 0.5052, val_auc: 0.7577 val_ks: 0.4310, test_auc: 0.7270 test_ks: 0.4054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 train_loss: 0.2090 train_acc: 0.9485, train_auc: 0.8336 train_ks: 0.5056, val_auc: 0.7573 val_ks: 0.4367, test_auc: 0.7275 test_ks: 0.4013\n",
      "Epoch 24 train_loss: 0.2006 train_acc: 0.9553, train_auc: 0.8359 train_ks: 0.5108, val_auc: 0.7536 val_ks: 0.4420, test_auc: 0.7241 test_ks: 0.3937\n",
      "Epoch 25 train_loss: 0.2005 train_acc: 0.9524, train_auc: 0.8367 train_ks: 0.5137, val_auc: 0.7539 val_ks: 0.4220, test_auc: 0.7297 test_ks: 0.4163\n",
      "Epoch 26 train_loss: 0.2164 train_acc: 0.9493, train_auc: 0.8388 train_ks: 0.5171, val_auc: 0.7581 val_ks: 0.4415, test_auc: 0.7323 test_ks: 0.4134\n",
      "Epoch 27 train_loss: 0.1966 train_acc: 0.9551, train_auc: 0.8418 train_ks: 0.5183, val_auc: 0.7598 val_ks: 0.4463, test_auc: 0.7235 test_ks: 0.4084\n",
      "Epoch 28 train_loss: 0.1998 train_acc: 0.9553, train_auc: 0.8413 train_ks: 0.5214, val_auc: 0.7566 val_ks: 0.4387, test_auc: 0.7257 test_ks: 0.4235\n",
      "Epoch 29 train_loss: 0.1972 train_acc: 0.9543, train_auc: 0.8435 train_ks: 0.5253, val_auc: 0.7593 val_ks: 0.4523, test_auc: 0.7202 test_ks: 0.4121\n",
      "Epoch 30 train_loss: 0.1841 train_acc: 0.9569, train_auc: 0.8440 train_ks: 0.5249, val_auc: 0.7556 val_ks: 0.4338, test_auc: 0.7220 test_ks: 0.4048\n",
      "\n",
      "5 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.2437 train_acc: 0.9549, train_auc: 0.7779 train_ks: 0.4238, val_auc: 0.7297 val_ks: 0.4043, test_auc: 0.7401 test_ks: 0.4091\n",
      "Epoch 2 train_loss: 0.1840 train_acc: 0.9695, train_auc: 0.7915 train_ks: 0.4428, val_auc: 0.7329 val_ks: 0.4098, test_auc: 0.7546 test_ks: 0.4341\n",
      "Epoch 3 train_loss: 0.1879 train_acc: 0.9686, train_auc: 0.8039 train_ks: 0.4584, val_auc: 0.7393 val_ks: 0.4057, test_auc: 0.7645 test_ks: 0.4322\n",
      "Epoch 4 train_loss: 0.1750 train_acc: 0.9713, train_auc: 0.8091 train_ks: 0.4644, val_auc: 0.7424 val_ks: 0.4010, test_auc: 0.7701 test_ks: 0.4198\n",
      "Epoch 5 train_loss: 0.1632 train_acc: 0.9740, train_auc: 0.8120 train_ks: 0.4708, val_auc: 0.7438 val_ks: 0.4292, test_auc: 0.7570 test_ks: 0.4052\n",
      "Epoch 6 train_loss: 0.1567 train_acc: 0.9769, train_auc: 0.8158 train_ks: 0.4806, val_auc: 0.7417 val_ks: 0.4016, test_auc: 0.7585 test_ks: 0.4135\n",
      "Epoch 7 train_loss: 0.1587 train_acc: 0.9748, train_auc: 0.8201 train_ks: 0.4930, val_auc: 0.7510 val_ks: 0.4113, test_auc: 0.7646 test_ks: 0.4163\n",
      "Epoch 8 train_loss: 0.1517 train_acc: 0.9770, train_auc: 0.8226 train_ks: 0.4986, val_auc: 0.7457 val_ks: 0.3913, test_auc: 0.7618 test_ks: 0.3995\n",
      "Epoch 9 train_loss: 0.1523 train_acc: 0.9746, train_auc: 0.8259 train_ks: 0.5078, val_auc: 0.7470 val_ks: 0.4134, test_auc: 0.7633 test_ks: 0.4128\n",
      "Epoch 10 train_loss: 0.1631 train_acc: 0.9713, train_auc: 0.8267 train_ks: 0.5084, val_auc: 0.7423 val_ks: 0.3971, test_auc: 0.7650 test_ks: 0.3964\n",
      "Epoch 11 train_loss: 0.1592 train_acc: 0.9724, train_auc: 0.8292 train_ks: 0.5099, val_auc: 0.7403 val_ks: 0.3761, test_auc: 0.7630 test_ks: 0.3975\n",
      "Epoch 12 train_loss: 0.1617 train_acc: 0.9723, train_auc: 0.8295 train_ks: 0.5112, val_auc: 0.7389 val_ks: 0.3811, test_auc: 0.7533 test_ks: 0.3981\n",
      "Epoch 13 train_loss: 0.1659 train_acc: 0.9696, train_auc: 0.8344 train_ks: 0.5318, val_auc: 0.7465 val_ks: 0.3964, test_auc: 0.7639 test_ks: 0.4196\n",
      "Epoch 14 train_loss: 0.1699 train_acc: 0.9705, train_auc: 0.8362 train_ks: 0.5348, val_auc: 0.7490 val_ks: 0.3853, test_auc: 0.7616 test_ks: 0.4125\n",
      "Epoch 15 train_loss: 0.1797 train_acc: 0.9622, train_auc: 0.8378 train_ks: 0.5333, val_auc: 0.7449 val_ks: 0.3988, test_auc: 0.7593 test_ks: 0.4162\n",
      "Epoch 16 train_loss: 0.1795 train_acc: 0.9650, train_auc: 0.8384 train_ks: 0.5315, val_auc: 0.7407 val_ks: 0.3867, test_auc: 0.7537 test_ks: 0.4029\n",
      "Epoch 17 train_loss: 0.1787 train_acc: 0.9630, train_auc: 0.8409 train_ks: 0.5428, val_auc: 0.7388 val_ks: 0.3904, test_auc: 0.7595 test_ks: 0.4033\n",
      "Epoch 18 train_loss: 0.1709 train_acc: 0.9655, train_auc: 0.8428 train_ks: 0.5382, val_auc: 0.7423 val_ks: 0.3873, test_auc: 0.7573 test_ks: 0.4140\n",
      "Epoch 19 train_loss: 0.1600 train_acc: 0.9703, train_auc: 0.8453 train_ks: 0.5450, val_auc: 0.7440 val_ks: 0.3930, test_auc: 0.7608 test_ks: 0.4237\n",
      "Epoch 20 train_loss: 0.1763 train_acc: 0.9639, train_auc: 0.8475 train_ks: 0.5501, val_auc: 0.7425 val_ks: 0.4021, test_auc: 0.7616 test_ks: 0.4060\n",
      "Epoch 21 train_loss: 0.1633 train_acc: 0.9677, train_auc: 0.8481 train_ks: 0.5521, val_auc: 0.7398 val_ks: 0.3920, test_auc: 0.7612 test_ks: 0.4272\n",
      "Epoch 22 train_loss: 0.1692 train_acc: 0.9658, train_auc: 0.8501 train_ks: 0.5501, val_auc: 0.7421 val_ks: 0.3964, test_auc: 0.7610 test_ks: 0.4180\n",
      "Epoch 23 train_loss: 0.1739 train_acc: 0.9651, train_auc: 0.8519 train_ks: 0.5567, val_auc: 0.7389 val_ks: 0.4013, test_auc: 0.7617 test_ks: 0.4317\n",
      "Epoch 24 train_loss: 0.1614 train_acc: 0.9681, train_auc: 0.8528 train_ks: 0.5566, val_auc: 0.7379 val_ks: 0.4186, test_auc: 0.7539 test_ks: 0.3945\n",
      "Epoch 25 train_loss: 0.1704 train_acc: 0.9636, train_auc: 0.8536 train_ks: 0.5579, val_auc: 0.7352 val_ks: 0.4000, test_auc: 0.7589 test_ks: 0.4082\n",
      "Epoch 26 train_loss: 0.1629 train_acc: 0.9662, train_auc: 0.8546 train_ks: 0.5528, val_auc: 0.7320 val_ks: 0.3961, test_auc: 0.7559 test_ks: 0.4178\n",
      "Epoch 27 train_loss: 0.1574 train_acc: 0.9692, train_auc: 0.8566 train_ks: 0.5566, val_auc: 0.7314 val_ks: 0.3943, test_auc: 0.7597 test_ks: 0.4177\n",
      "Epoch 28 train_loss: 0.1631 train_acc: 0.9644, train_auc: 0.8568 train_ks: 0.5604, val_auc: 0.7364 val_ks: 0.3894, test_auc: 0.7629 test_ks: 0.4151\n",
      "Epoch 29 train_loss: 0.1691 train_acc: 0.9636, train_auc: 0.8602 train_ks: 0.5625, val_auc: 0.7303 val_ks: 0.3903, test_auc: 0.7633 test_ks: 0.3967\n",
      "Epoch 30 train_loss: 0.1577 train_acc: 0.9659, train_auc: 0.8600 train_ks: 0.5677, val_auc: 0.7285 val_ks: 0.3658, test_auc: 0.7555 test_ks: 0.3936\n",
      "\n",
      "6 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.2253 train_acc: 0.9517, train_auc: 0.7680 train_ks: 0.4226, val_auc: 0.8043 val_ks: 0.4744, test_auc: 0.7357 test_ks: 0.3711\n",
      "Epoch 2 train_loss: 0.1896 train_acc: 0.9587, train_auc: 0.7804 train_ks: 0.4436, val_auc: 0.8059 val_ks: 0.4711, test_auc: 0.7485 test_ks: 0.3890\n",
      "Epoch 3 train_loss: 0.1777 train_acc: 0.9616, train_auc: 0.7863 train_ks: 0.4563, val_auc: 0.8101 val_ks: 0.4917, test_auc: 0.7554 test_ks: 0.4089\n",
      "Epoch 4 train_loss: 0.1491 train_acc: 0.9685, train_auc: 0.7898 train_ks: 0.4622, val_auc: 0.8201 val_ks: 0.5140, test_auc: 0.7522 test_ks: 0.4084\n",
      "Epoch 5 train_loss: 0.1533 train_acc: 0.9673, train_auc: 0.7937 train_ks: 0.4619, val_auc: 0.8174 val_ks: 0.4959, test_auc: 0.7556 test_ks: 0.4200\n",
      "Epoch 6 train_loss: 0.1716 train_acc: 0.9627, train_auc: 0.7975 train_ks: 0.4655, val_auc: 0.8199 val_ks: 0.5220, test_auc: 0.7543 test_ks: 0.4255\n",
      "Epoch 7 train_loss: 0.1479 train_acc: 0.9709, train_auc: 0.7995 train_ks: 0.4725, val_auc: 0.8159 val_ks: 0.5290, test_auc: 0.7518 test_ks: 0.4165\n",
      "Epoch 8 train_loss: 0.1675 train_acc: 0.9624, train_auc: 0.8000 train_ks: 0.4654, val_auc: 0.8211 val_ks: 0.5349, test_auc: 0.7529 test_ks: 0.4370\n",
      "Epoch 9 train_loss: 0.1673 train_acc: 0.9651, train_auc: 0.8052 train_ks: 0.4668, val_auc: 0.8123 val_ks: 0.5182, test_auc: 0.7530 test_ks: 0.4192\n",
      "Epoch 10 train_loss: 0.1784 train_acc: 0.9603, train_auc: 0.8072 train_ks: 0.4721, val_auc: 0.8103 val_ks: 0.5403, test_auc: 0.7537 test_ks: 0.4283\n",
      "Epoch 11 train_loss: 0.1631 train_acc: 0.9636, train_auc: 0.8082 train_ks: 0.4710, val_auc: 0.8181 val_ks: 0.5375, test_auc: 0.7546 test_ks: 0.4450\n",
      "Epoch 12 train_loss: 0.1541 train_acc: 0.9698, train_auc: 0.8130 train_ks: 0.4835, val_auc: 0.8088 val_ks: 0.5241, test_auc: 0.7508 test_ks: 0.4104\n",
      "Epoch 13 train_loss: 0.1506 train_acc: 0.9706, train_auc: 0.8151 train_ks: 0.4859, val_auc: 0.8178 val_ks: 0.5531, test_auc: 0.7560 test_ks: 0.4170\n",
      "Epoch 14 train_loss: 0.1589 train_acc: 0.9653, train_auc: 0.8158 train_ks: 0.4828, val_auc: 0.8144 val_ks: 0.5357, test_auc: 0.7537 test_ks: 0.4283\n",
      "Epoch 15 train_loss: 0.1567 train_acc: 0.9658, train_auc: 0.8181 train_ks: 0.4859, val_auc: 0.8149 val_ks: 0.5345, test_auc: 0.7534 test_ks: 0.4069\n",
      "Epoch 16 train_loss: 0.1508 train_acc: 0.9708, train_auc: 0.8216 train_ks: 0.4985, val_auc: 0.8074 val_ks: 0.5311, test_auc: 0.7480 test_ks: 0.4018\n",
      "Epoch 17 train_loss: 0.1743 train_acc: 0.9606, train_auc: 0.8213 train_ks: 0.4911, val_auc: 0.8065 val_ks: 0.5273, test_auc: 0.7505 test_ks: 0.4266\n",
      "Epoch 18 train_loss: 0.1620 train_acc: 0.9663, train_auc: 0.8238 train_ks: 0.4931, val_auc: 0.8057 val_ks: 0.5319, test_auc: 0.7507 test_ks: 0.4112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 train_loss: 0.1807 train_acc: 0.9590, train_auc: 0.8247 train_ks: 0.5053, val_auc: 0.8072 val_ks: 0.5297, test_auc: 0.7541 test_ks: 0.4307\n",
      "Epoch 20 train_loss: 0.1816 train_acc: 0.9575, train_auc: 0.8243 train_ks: 0.4974, val_auc: 0.8033 val_ks: 0.5308, test_auc: 0.7493 test_ks: 0.4187\n",
      "Epoch 21 train_loss: 0.1862 train_acc: 0.9552, train_auc: 0.8268 train_ks: 0.5025, val_auc: 0.8071 val_ks: 0.5333, test_auc: 0.7509 test_ks: 0.4072\n",
      "Epoch 22 train_loss: 0.1779 train_acc: 0.9570, train_auc: 0.8274 train_ks: 0.5031, val_auc: 0.8065 val_ks: 0.5251, test_auc: 0.7488 test_ks: 0.4071\n",
      "Epoch 23 train_loss: 0.1705 train_acc: 0.9625, train_auc: 0.8299 train_ks: 0.5078, val_auc: 0.7999 val_ks: 0.5292, test_auc: 0.7450 test_ks: 0.4020\n",
      "Epoch 24 train_loss: 0.1784 train_acc: 0.9579, train_auc: 0.8294 train_ks: 0.5075, val_auc: 0.8000 val_ks: 0.5187, test_auc: 0.7457 test_ks: 0.3944\n",
      "Epoch 25 train_loss: 0.1760 train_acc: 0.9598, train_auc: 0.8334 train_ks: 0.5158, val_auc: 0.7958 val_ks: 0.5150, test_auc: 0.7444 test_ks: 0.3976\n",
      "Epoch 26 train_loss: 0.1653 train_acc: 0.9638, train_auc: 0.8331 train_ks: 0.5140, val_auc: 0.7922 val_ks: 0.5047, test_auc: 0.7423 test_ks: 0.3857\n",
      "Epoch 27 train_loss: 0.1610 train_acc: 0.9643, train_auc: 0.8356 train_ks: 0.5175, val_auc: 0.7939 val_ks: 0.5052, test_auc: 0.7406 test_ks: 0.4023\n",
      "Epoch 28 train_loss: 0.1573 train_acc: 0.9653, train_auc: 0.8348 train_ks: 0.5206, val_auc: 0.7901 val_ks: 0.5010, test_auc: 0.7390 test_ks: 0.3876\n",
      "Epoch 29 train_loss: 0.1517 train_acc: 0.9639, train_auc: 0.8355 train_ks: 0.5234, val_auc: 0.7935 val_ks: 0.5162, test_auc: 0.7396 test_ks: 0.3999\n",
      "Epoch 30 train_loss: 0.1432 train_acc: 0.9694, train_auc: 0.8388 train_ks: 0.5276, val_auc: 0.7892 val_ks: 0.5005, test_auc: 0.7385 test_ks: 0.4220\n",
      "\n",
      "7 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.0913 train_acc: 0.9885, train_auc: 0.7750 train_ks: 0.4114, val_auc: 0.7672 val_ks: 0.4806, test_auc: 0.7112 test_ks: 0.3231\n",
      "Epoch 2 train_loss: 0.1084 train_acc: 0.9844, train_auc: 0.7847 train_ks: 0.4269, val_auc: 0.7674 val_ks: 0.4910, test_auc: 0.7100 test_ks: 0.3277\n",
      "Epoch 3 train_loss: 0.1056 train_acc: 0.9821, train_auc: 0.8007 train_ks: 0.4547, val_auc: 0.7748 val_ks: 0.4744, test_auc: 0.7261 test_ks: 0.3773\n",
      "Epoch 4 train_loss: 0.1091 train_acc: 0.9839, train_auc: 0.8080 train_ks: 0.4663, val_auc: 0.7776 val_ks: 0.4737, test_auc: 0.7309 test_ks: 0.3575\n",
      "Epoch 5 train_loss: 0.1241 train_acc: 0.9812, train_auc: 0.8095 train_ks: 0.4687, val_auc: 0.7711 val_ks: 0.4709, test_auc: 0.7288 test_ks: 0.3638\n",
      "Epoch 6 train_loss: 0.1169 train_acc: 0.9822, train_auc: 0.8111 train_ks: 0.4709, val_auc: 0.7721 val_ks: 0.4766, test_auc: 0.7314 test_ks: 0.3808\n",
      "Epoch 7 train_loss: 0.1163 train_acc: 0.9800, train_auc: 0.8126 train_ks: 0.4797, val_auc: 0.7723 val_ks: 0.4646, test_auc: 0.7320 test_ks: 0.3728\n",
      "Epoch 8 train_loss: 0.1302 train_acc: 0.9758, train_auc: 0.8176 train_ks: 0.4865, val_auc: 0.7718 val_ks: 0.4699, test_auc: 0.7372 test_ks: 0.3660\n",
      "Epoch 9 train_loss: 0.1260 train_acc: 0.9791, train_auc: 0.8217 train_ks: 0.5010, val_auc: 0.7717 val_ks: 0.4433, test_auc: 0.7380 test_ks: 0.3733\n",
      "Epoch 10 train_loss: 0.1343 train_acc: 0.9773, train_auc: 0.8225 train_ks: 0.4950, val_auc: 0.7702 val_ks: 0.4557, test_auc: 0.7376 test_ks: 0.3850\n",
      "Epoch 11 train_loss: 0.1336 train_acc: 0.9767, train_auc: 0.8226 train_ks: 0.4947, val_auc: 0.7674 val_ks: 0.4400, test_auc: 0.7401 test_ks: 0.3849\n",
      "Epoch 12 train_loss: 0.1401 train_acc: 0.9722, train_auc: 0.8258 train_ks: 0.5081, val_auc: 0.7689 val_ks: 0.4571, test_auc: 0.7395 test_ks: 0.3682\n",
      "Epoch 13 train_loss: 0.1444 train_acc: 0.9726, train_auc: 0.8274 train_ks: 0.5066, val_auc: 0.7634 val_ks: 0.4433, test_auc: 0.7406 test_ks: 0.3717\n",
      "Epoch 14 train_loss: 0.1411 train_acc: 0.9726, train_auc: 0.8282 train_ks: 0.5108, val_auc: 0.7621 val_ks: 0.4379, test_auc: 0.7376 test_ks: 0.3737\n",
      "Epoch 15 train_loss: 0.1376 train_acc: 0.9730, train_auc: 0.8302 train_ks: 0.5118, val_auc: 0.7633 val_ks: 0.4445, test_auc: 0.7422 test_ks: 0.3692\n",
      "Epoch 16 train_loss: 0.1465 train_acc: 0.9721, train_auc: 0.8314 train_ks: 0.5136, val_auc: 0.7619 val_ks: 0.4316, test_auc: 0.7428 test_ks: 0.3718\n",
      "Epoch 17 train_loss: 0.1472 train_acc: 0.9708, train_auc: 0.8337 train_ks: 0.5204, val_auc: 0.7592 val_ks: 0.4296, test_auc: 0.7428 test_ks: 0.3782\n",
      "Epoch 18 train_loss: 0.1466 train_acc: 0.9714, train_auc: 0.8352 train_ks: 0.5244, val_auc: 0.7610 val_ks: 0.4271, test_auc: 0.7419 test_ks: 0.3715\n",
      "Epoch 19 train_loss: 0.1540 train_acc: 0.9683, train_auc: 0.8373 train_ks: 0.5345, val_auc: 0.7583 val_ks: 0.4192, test_auc: 0.7398 test_ks: 0.3673\n",
      "Epoch 20 train_loss: 0.1556 train_acc: 0.9675, train_auc: 0.8377 train_ks: 0.5239, val_auc: 0.7566 val_ks: 0.4184, test_auc: 0.7418 test_ks: 0.3731\n",
      "Epoch 21 train_loss: 0.1425 train_acc: 0.9720, train_auc: 0.8388 train_ks: 0.5340, val_auc: 0.7549 val_ks: 0.4105, test_auc: 0.7386 test_ks: 0.3646\n",
      "Epoch 22 train_loss: 0.1430 train_acc: 0.9716, train_auc: 0.8409 train_ks: 0.5393, val_auc: 0.7582 val_ks: 0.4219, test_auc: 0.7410 test_ks: 0.3695\n",
      "Epoch 23 train_loss: 0.1514 train_acc: 0.9687, train_auc: 0.8423 train_ks: 0.5374, val_auc: 0.7549 val_ks: 0.4111, test_auc: 0.7428 test_ks: 0.3761\n",
      "Epoch 24 train_loss: 0.1349 train_acc: 0.9729, train_auc: 0.8434 train_ks: 0.5470, val_auc: 0.7545 val_ks: 0.4209, test_auc: 0.7416 test_ks: 0.3937\n",
      "Epoch 25 train_loss: 0.1400 train_acc: 0.9723, train_auc: 0.8462 train_ks: 0.5469, val_auc: 0.7552 val_ks: 0.4170, test_auc: 0.7401 test_ks: 0.3630\n",
      "Epoch 26 train_loss: 0.1320 train_acc: 0.9753, train_auc: 0.8467 train_ks: 0.5499, val_auc: 0.7560 val_ks: 0.4307, test_auc: 0.7416 test_ks: 0.3632\n",
      "Epoch 27 train_loss: 0.1341 train_acc: 0.9747, train_auc: 0.8474 train_ks: 0.5414, val_auc: 0.7511 val_ks: 0.4046, test_auc: 0.7421 test_ks: 0.3792\n",
      "Epoch 28 train_loss: 0.1386 train_acc: 0.9724, train_auc: 0.8485 train_ks: 0.5493, val_auc: 0.7488 val_ks: 0.4102, test_auc: 0.7380 test_ks: 0.3737\n",
      "Epoch 29 train_loss: 0.1369 train_acc: 0.9726, train_auc: 0.8487 train_ks: 0.5548, val_auc: 0.7510 val_ks: 0.4086, test_auc: 0.7376 test_ks: 0.3603\n",
      "Epoch 30 train_loss: 0.1344 train_acc: 0.9726, train_auc: 0.8490 train_ks: 0.5524, val_auc: 0.7493 val_ks: 0.4150, test_auc: 0.7336 test_ks: 0.3797\n",
      "\n",
      "8 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.1165 train_acc: 0.9843, train_auc: 0.7855 train_ks: 0.4295, val_auc: 0.7306 val_ks: 0.3396, test_auc: 0.7742 test_ks: 0.4656\n",
      "Epoch 2 train_loss: 0.1255 train_acc: 0.9811, train_auc: 0.7972 train_ks: 0.4601, val_auc: 0.7288 val_ks: 0.3210, test_auc: 0.7878 test_ks: 0.4904\n",
      "Epoch 3 train_loss: 0.1256 train_acc: 0.9809, train_auc: 0.8041 train_ks: 0.4714, val_auc: 0.7359 val_ks: 0.3427, test_auc: 0.7910 test_ks: 0.5053\n",
      "Epoch 4 train_loss: 0.1463 train_acc: 0.9754, train_auc: 0.8126 train_ks: 0.4873, val_auc: 0.7314 val_ks: 0.3364, test_auc: 0.7930 test_ks: 0.4957\n",
      "Epoch 5 train_loss: 0.1530 train_acc: 0.9729, train_auc: 0.8151 train_ks: 0.4978, val_auc: 0.7287 val_ks: 0.3317, test_auc: 0.7934 test_ks: 0.4899\n",
      "Epoch 6 train_loss: 0.1836 train_acc: 0.9629, train_auc: 0.8180 train_ks: 0.4950, val_auc: 0.7316 val_ks: 0.3513, test_auc: 0.7920 test_ks: 0.4946\n",
      "Epoch 7 train_loss: 0.2389 train_acc: 0.9487, train_auc: 0.8207 train_ks: 0.5050, val_auc: 0.7343 val_ks: 0.3571, test_auc: 0.7905 test_ks: 0.4730\n",
      "Epoch 8 train_loss: 0.1643 train_acc: 0.9686, train_auc: 0.8232 train_ks: 0.5061, val_auc: 0.7318 val_ks: 0.3455, test_auc: 0.7870 test_ks: 0.4665\n",
      "Epoch 9 train_loss: 0.1936 train_acc: 0.9586, train_auc: 0.8247 train_ks: 0.5080, val_auc: 0.7296 val_ks: 0.3350, test_auc: 0.7862 test_ks: 0.4626\n",
      "Epoch 10 train_loss: 0.2111 train_acc: 0.9481, train_auc: 0.8273 train_ks: 0.5135, val_auc: 0.7302 val_ks: 0.3486, test_auc: 0.7846 test_ks: 0.4441\n",
      "Epoch 11 train_loss: 0.2242 train_acc: 0.9442, train_auc: 0.8292 train_ks: 0.5169, val_auc: 0.7283 val_ks: 0.3641, test_auc: 0.7854 test_ks: 0.4469\n",
      "Epoch 12 train_loss: 0.2191 train_acc: 0.9465, train_auc: 0.8303 train_ks: 0.5192, val_auc: 0.7350 val_ks: 0.3564, test_auc: 0.7858 test_ks: 0.4611\n",
      "Epoch 13 train_loss: 0.2293 train_acc: 0.9457, train_auc: 0.8337 train_ks: 0.5287, val_auc: 0.7341 val_ks: 0.3567, test_auc: 0.7864 test_ks: 0.4568\n",
      "Epoch 14 train_loss: 0.2090 train_acc: 0.9500, train_auc: 0.8343 train_ks: 0.5288, val_auc: 0.7340 val_ks: 0.3803, test_auc: 0.7812 test_ks: 0.4373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 train_loss: 0.1961 train_acc: 0.9545, train_auc: 0.8362 train_ks: 0.5333, val_auc: 0.7318 val_ks: 0.3522, test_auc: 0.7815 test_ks: 0.4566\n",
      "Epoch 16 train_loss: 0.2116 train_acc: 0.9474, train_auc: 0.8382 train_ks: 0.5368, val_auc: 0.7288 val_ks: 0.3439, test_auc: 0.7769 test_ks: 0.4678\n",
      "Epoch 17 train_loss: 0.2126 train_acc: 0.9474, train_auc: 0.8400 train_ks: 0.5410, val_auc: 0.7341 val_ks: 0.3548, test_auc: 0.7817 test_ks: 0.4518\n",
      "Epoch 18 train_loss: 0.1970 train_acc: 0.9541, train_auc: 0.8413 train_ks: 0.5407, val_auc: 0.7337 val_ks: 0.3546, test_auc: 0.7820 test_ks: 0.4637\n",
      "Epoch 19 train_loss: 0.1877 train_acc: 0.9560, train_auc: 0.8428 train_ks: 0.5479, val_auc: 0.7348 val_ks: 0.3598, test_auc: 0.7822 test_ks: 0.4785\n",
      "Epoch 20 train_loss: 0.1937 train_acc: 0.9548, train_auc: 0.8446 train_ks: 0.5488, val_auc: 0.7372 val_ks: 0.3590, test_auc: 0.7813 test_ks: 0.4516\n",
      "Epoch 21 train_loss: 0.1745 train_acc: 0.9615, train_auc: 0.8462 train_ks: 0.5482, val_auc: 0.7360 val_ks: 0.3536, test_auc: 0.7813 test_ks: 0.4676\n",
      "Epoch 22 train_loss: 0.1894 train_acc: 0.9544, train_auc: 0.8479 train_ks: 0.5497, val_auc: 0.7377 val_ks: 0.3605, test_auc: 0.7815 test_ks: 0.4621\n",
      "Epoch 23 train_loss: 0.1751 train_acc: 0.9616, train_auc: 0.8483 train_ks: 0.5535, val_auc: 0.7354 val_ks: 0.3710, test_auc: 0.7807 test_ks: 0.4675\n",
      "Epoch 24 train_loss: 0.1727 train_acc: 0.9611, train_auc: 0.8492 train_ks: 0.5561, val_auc: 0.7377 val_ks: 0.3584, test_auc: 0.7715 test_ks: 0.4368\n",
      "Epoch 25 train_loss: 0.1557 train_acc: 0.9666, train_auc: 0.8509 train_ks: 0.5585, val_auc: 0.7342 val_ks: 0.3566, test_auc: 0.7759 test_ks: 0.4611\n",
      "Epoch 26 train_loss: 0.1563 train_acc: 0.9664, train_auc: 0.8531 train_ks: 0.5685, val_auc: 0.7322 val_ks: 0.3703, test_auc: 0.7771 test_ks: 0.4581\n",
      "Epoch 27 train_loss: 0.1571 train_acc: 0.9652, train_auc: 0.8542 train_ks: 0.5665, val_auc: 0.7370 val_ks: 0.3577, test_auc: 0.7792 test_ks: 0.4615\n",
      "Epoch 28 train_loss: 0.1476 train_acc: 0.9671, train_auc: 0.8553 train_ks: 0.5644, val_auc: 0.7314 val_ks: 0.3648, test_auc: 0.7803 test_ks: 0.4582\n",
      "Epoch 29 train_loss: 0.1413 train_acc: 0.9702, train_auc: 0.8549 train_ks: 0.5699, val_auc: 0.7368 val_ks: 0.3643, test_auc: 0.7794 test_ks: 0.4783\n",
      "Epoch 30 train_loss: 0.1487 train_acc: 0.9677, train_auc: 0.8574 train_ks: 0.5757, val_auc: 0.7326 val_ks: 0.3660, test_auc: 0.7761 test_ks: 0.4649\n",
      "\n",
      "9 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.1192 train_acc: 0.9816, train_auc: 0.7768 train_ks: 0.4213, val_auc: 0.7290 val_ks: 0.3318, test_auc: 0.7621 test_ks: 0.3997\n",
      "Epoch 2 train_loss: 0.1120 train_acc: 0.9810, train_auc: 0.7902 train_ks: 0.4451, val_auc: 0.7417 val_ks: 0.3535, test_auc: 0.7668 test_ks: 0.4161\n",
      "Epoch 3 train_loss: 0.1192 train_acc: 0.9797, train_auc: 0.7988 train_ks: 0.4630, val_auc: 0.7487 val_ks: 0.3637, test_auc: 0.7694 test_ks: 0.4458\n",
      "Epoch 4 train_loss: 0.1129 train_acc: 0.9801, train_auc: 0.7974 train_ks: 0.4595, val_auc: 0.7471 val_ks: 0.3783, test_auc: 0.7692 test_ks: 0.4485\n",
      "Epoch 5 train_loss: 0.1112 train_acc: 0.9802, train_auc: 0.8030 train_ks: 0.4646, val_auc: 0.7492 val_ks: 0.3716, test_auc: 0.7663 test_ks: 0.4246\n",
      "Epoch 6 train_loss: 0.1222 train_acc: 0.9778, train_auc: 0.8072 train_ks: 0.4773, val_auc: 0.7523 val_ks: 0.3726, test_auc: 0.7734 test_ks: 0.4410\n",
      "Epoch 7 train_loss: 0.1253 train_acc: 0.9763, train_auc: 0.8097 train_ks: 0.4789, val_auc: 0.7517 val_ks: 0.3734, test_auc: 0.7759 test_ks: 0.4521\n",
      "Epoch 8 train_loss: 0.1379 train_acc: 0.9732, train_auc: 0.8114 train_ks: 0.4809, val_auc: 0.7466 val_ks: 0.3573, test_auc: 0.7771 test_ks: 0.4430\n",
      "Epoch 9 train_loss: 0.1315 train_acc: 0.9724, train_auc: 0.8152 train_ks: 0.4849, val_auc: 0.7500 val_ks: 0.3764, test_auc: 0.7753 test_ks: 0.4543\n",
      "Epoch 10 train_loss: 0.1326 train_acc: 0.9724, train_auc: 0.8168 train_ks: 0.4963, val_auc: 0.7426 val_ks: 0.3680, test_auc: 0.7743 test_ks: 0.4616\n",
      "Epoch 11 train_loss: 0.1310 train_acc: 0.9746, train_auc: 0.8184 train_ks: 0.4933, val_auc: 0.7475 val_ks: 0.3922, test_auc: 0.7755 test_ks: 0.4532\n",
      "Epoch 12 train_loss: 0.1350 train_acc: 0.9707, train_auc: 0.8203 train_ks: 0.5007, val_auc: 0.7482 val_ks: 0.3710, test_auc: 0.7784 test_ks: 0.4598\n",
      "Epoch 13 train_loss: 0.1632 train_acc: 0.9594, train_auc: 0.8200 train_ks: 0.5005, val_auc: 0.7451 val_ks: 0.3773, test_auc: 0.7782 test_ks: 0.4523\n",
      "Epoch 14 train_loss: 0.1500 train_acc: 0.9670, train_auc: 0.8237 train_ks: 0.5009, val_auc: 0.7471 val_ks: 0.3660, test_auc: 0.7783 test_ks: 0.4622\n",
      "Epoch 15 train_loss: 0.1478 train_acc: 0.9679, train_auc: 0.8241 train_ks: 0.5049, val_auc: 0.7437 val_ks: 0.3429, test_auc: 0.7757 test_ks: 0.4559\n",
      "Epoch 16 train_loss: 0.1633 train_acc: 0.9622, train_auc: 0.8257 train_ks: 0.5052, val_auc: 0.7440 val_ks: 0.3577, test_auc: 0.7776 test_ks: 0.4636\n",
      "Epoch 17 train_loss: 0.1620 train_acc: 0.9633, train_auc: 0.8282 train_ks: 0.5093, val_auc: 0.7445 val_ks: 0.3593, test_auc: 0.7771 test_ks: 0.4653\n",
      "Epoch 18 train_loss: 0.1822 train_acc: 0.9534, train_auc: 0.8283 train_ks: 0.5107, val_auc: 0.7416 val_ks: 0.3568, test_auc: 0.7773 test_ks: 0.4738\n",
      "Epoch 19 train_loss: 0.1770 train_acc: 0.9581, train_auc: 0.8296 train_ks: 0.5147, val_auc: 0.7384 val_ks: 0.3483, test_auc: 0.7761 test_ks: 0.4729\n",
      "Epoch 20 train_loss: 0.1663 train_acc: 0.9613, train_auc: 0.8306 train_ks: 0.5135, val_auc: 0.7338 val_ks: 0.3439, test_auc: 0.7718 test_ks: 0.4677\n",
      "Epoch 21 train_loss: 0.1753 train_acc: 0.9587, train_auc: 0.8340 train_ks: 0.5166, val_auc: 0.7333 val_ks: 0.3357, test_auc: 0.7733 test_ks: 0.4677\n",
      "Epoch 22 train_loss: 0.1740 train_acc: 0.9580, train_auc: 0.8350 train_ks: 0.5200, val_auc: 0.7296 val_ks: 0.3556, test_auc: 0.7722 test_ks: 0.4605\n",
      "Epoch 23 train_loss: 0.1779 train_acc: 0.9589, train_auc: 0.8355 train_ks: 0.5200, val_auc: 0.7316 val_ks: 0.3402, test_auc: 0.7705 test_ks: 0.4575\n",
      "Epoch 24 train_loss: 0.1710 train_acc: 0.9614, train_auc: 0.8360 train_ks: 0.5202, val_auc: 0.7316 val_ks: 0.3468, test_auc: 0.7695 test_ks: 0.4670\n",
      "Epoch 25 train_loss: 0.1601 train_acc: 0.9650, train_auc: 0.8396 train_ks: 0.5293, val_auc: 0.7277 val_ks: 0.3313, test_auc: 0.7689 test_ks: 0.4618\n",
      "Epoch 26 train_loss: 0.1579 train_acc: 0.9648, train_auc: 0.8395 train_ks: 0.5281, val_auc: 0.7241 val_ks: 0.3275, test_auc: 0.7708 test_ks: 0.4635\n",
      "Epoch 27 train_loss: 0.1576 train_acc: 0.9647, train_auc: 0.8418 train_ks: 0.5296, val_auc: 0.7274 val_ks: 0.3261, test_auc: 0.7681 test_ks: 0.4587\n",
      "Epoch 28 train_loss: 0.1767 train_acc: 0.9585, train_auc: 0.8411 train_ks: 0.5303, val_auc: 0.7144 val_ks: 0.3162, test_auc: 0.7659 test_ks: 0.4635\n",
      "Epoch 29 train_loss: 0.1604 train_acc: 0.9647, train_auc: 0.8428 train_ks: 0.5315, val_auc: 0.7198 val_ks: 0.3384, test_auc: 0.7705 test_ks: 0.4563\n",
      "Epoch 30 train_loss: 0.1525 train_acc: 0.9658, train_auc: 0.8444 train_ks: 0.5315, val_auc: 0.7173 val_ks: 0.3239, test_auc: 0.7680 test_ks: 0.4493\n",
      "\n",
      "10 fold start..............\n",
      "\n",
      "Epoch 1 train_loss: 0.1794 train_acc: 0.9641, train_auc: 0.7617 train_ks: 0.4034, val_auc: 0.7227 val_ks: 0.3639, test_auc: 0.8174 test_ks: 0.5276\n",
      "Epoch 2 train_loss: 0.1634 train_acc: 0.9706, train_auc: 0.7775 train_ks: 0.4211, val_auc: 0.7303 val_ks: 0.3817, test_auc: 0.8298 test_ks: 0.5510\n",
      "Epoch 3 train_loss: 0.1583 train_acc: 0.9659, train_auc: 0.7846 train_ks: 0.4355, val_auc: 0.7387 val_ks: 0.4046, test_auc: 0.8311 test_ks: 0.5464\n",
      "Epoch 4 train_loss: 0.1559 train_acc: 0.9705, train_auc: 0.7929 train_ks: 0.4550, val_auc: 0.7456 val_ks: 0.4166, test_auc: 0.8386 test_ks: 0.5709\n",
      "Epoch 5 train_loss: 0.1529 train_acc: 0.9701, train_auc: 0.7966 train_ks: 0.4512, val_auc: 0.7454 val_ks: 0.4320, test_auc: 0.8323 test_ks: 0.5465\n",
      "Epoch 6 train_loss: 0.1388 train_acc: 0.9780, train_auc: 0.8038 train_ks: 0.4733, val_auc: 0.7556 val_ks: 0.4498, test_auc: 0.8346 test_ks: 0.5396\n",
      "Epoch 7 train_loss: 0.1698 train_acc: 0.9706, train_auc: 0.8037 train_ks: 0.4670, val_auc: 0.7577 val_ks: 0.4481, test_auc: 0.8269 test_ks: 0.5300\n",
      "Epoch 8 train_loss: 0.1614 train_acc: 0.9732, train_auc: 0.8067 train_ks: 0.4708, val_auc: 0.7608 val_ks: 0.4465, test_auc: 0.8330 test_ks: 0.5569\n",
      "Epoch 9 train_loss: 0.1357 train_acc: 0.9787, train_auc: 0.8108 train_ks: 0.4811, val_auc: 0.7535 val_ks: 0.4441, test_auc: 0.8288 test_ks: 0.5397\n",
      "Epoch 10 train_loss: 0.1645 train_acc: 0.9699, train_auc: 0.8129 train_ks: 0.4818, val_auc: 0.7531 val_ks: 0.4421, test_auc: 0.8317 test_ks: 0.5474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train_loss: 0.1677 train_acc: 0.9689, train_auc: 0.8126 train_ks: 0.4825, val_auc: 0.7558 val_ks: 0.4298, test_auc: 0.8353 test_ks: 0.5383\n",
      "Epoch 12 train_loss: 0.1541 train_acc: 0.9717, train_auc: 0.8151 train_ks: 0.4906, val_auc: 0.7637 val_ks: 0.4469, test_auc: 0.8307 test_ks: 0.5490\n",
      "Epoch 13 train_loss: 0.1550 train_acc: 0.9721, train_auc: 0.8185 train_ks: 0.4965, val_auc: 0.7657 val_ks: 0.4485, test_auc: 0.8332 test_ks: 0.5421\n",
      "Epoch 14 train_loss: 0.1435 train_acc: 0.9740, train_auc: 0.8183 train_ks: 0.4860, val_auc: 0.7560 val_ks: 0.4253, test_auc: 0.8328 test_ks: 0.5512\n",
      "Epoch 15 train_loss: 0.1448 train_acc: 0.9740, train_auc: 0.8203 train_ks: 0.4960, val_auc: 0.7561 val_ks: 0.4442, test_auc: 0.8336 test_ks: 0.5554\n",
      "Epoch 16 train_loss: 0.1419 train_acc: 0.9735, train_auc: 0.8218 train_ks: 0.4959, val_auc: 0.7576 val_ks: 0.4415, test_auc: 0.8344 test_ks: 0.5402\n",
      "Epoch 17 train_loss: 0.1527 train_acc: 0.9717, train_auc: 0.8242 train_ks: 0.5040, val_auc: 0.7576 val_ks: 0.4431, test_auc: 0.8282 test_ks: 0.5345\n",
      "Epoch 18 train_loss: 0.1604 train_acc: 0.9690, train_auc: 0.8251 train_ks: 0.4996, val_auc: 0.7590 val_ks: 0.4663, test_auc: 0.8330 test_ks: 0.5449\n",
      "Epoch 19 train_loss: 0.1579 train_acc: 0.9698, train_auc: 0.8264 train_ks: 0.5083, val_auc: 0.7550 val_ks: 0.4474, test_auc: 0.8319 test_ks: 0.5396\n",
      "Epoch 20 train_loss: 0.1646 train_acc: 0.9658, train_auc: 0.8264 train_ks: 0.4981, val_auc: 0.7515 val_ks: 0.4564, test_auc: 0.8380 test_ks: 0.5569\n",
      "Epoch 21 train_loss: 0.1572 train_acc: 0.9689, train_auc: 0.8299 train_ks: 0.5093, val_auc: 0.7538 val_ks: 0.4570, test_auc: 0.8321 test_ks: 0.5561\n",
      "Epoch 22 train_loss: 0.1591 train_acc: 0.9665, train_auc: 0.8299 train_ks: 0.5071, val_auc: 0.7561 val_ks: 0.4587, test_auc: 0.8297 test_ks: 0.5523\n",
      "Epoch 23 train_loss: 0.1671 train_acc: 0.9652, train_auc: 0.8325 train_ks: 0.5109, val_auc: 0.7587 val_ks: 0.4420, test_auc: 0.8271 test_ks: 0.5705\n",
      "Epoch 24 train_loss: 0.1660 train_acc: 0.9677, train_auc: 0.8336 train_ks: 0.5082, val_auc: 0.7567 val_ks: 0.4431, test_auc: 0.8246 test_ks: 0.5578\n",
      "Epoch 25 train_loss: 0.1772 train_acc: 0.9627, train_auc: 0.8349 train_ks: 0.5127, val_auc: 0.7575 val_ks: 0.4571, test_auc: 0.8297 test_ks: 0.5527\n",
      "Epoch 26 train_loss: 0.1609 train_acc: 0.9687, train_auc: 0.8371 train_ks: 0.5140, val_auc: 0.7588 val_ks: 0.4483, test_auc: 0.8312 test_ks: 0.5580\n",
      "Epoch 27 train_loss: 0.1708 train_acc: 0.9644, train_auc: 0.8374 train_ks: 0.5209, val_auc: 0.7565 val_ks: 0.4507, test_auc: 0.8251 test_ks: 0.5297\n",
      "Epoch 28 train_loss: 0.1753 train_acc: 0.9644, train_auc: 0.8404 train_ks: 0.5224, val_auc: 0.7571 val_ks: 0.4389, test_auc: 0.8237 test_ks: 0.5401\n",
      "Epoch 29 train_loss: 0.1748 train_acc: 0.9615, train_auc: 0.8400 train_ks: 0.5220, val_auc: 0.7576 val_ks: 0.4753, test_auc: 0.8268 test_ks: 0.5332\n",
      "Epoch 30 train_loss: 0.1708 train_acc: 0.9628, train_auc: 0.8400 train_ks: 0.5185, val_auc: 0.7595 val_ks: 0.4600, test_auc: 0.8249 test_ks: 0.5453\n",
      "\n",
      "\n",
      "10 fold train_auc_avg: 0.8099 train_ks_avg: 0.4750, val_auc_avg: 0.7715 val_ks_avg: 0.4675, test_auc_avg: 0.7663 test_ks_avg: 0.4267\n"
     ]
    }
   ],
   "source": [
    "best_train_auc_record = []\n",
    "best_train_ks_record = []\n",
    "best_val_auc_record = []\n",
    "best_val_ks_record = []\n",
    "best_test_auc_record = []\n",
    "best_test_ks_record = []\n",
    "\n",
    "fold = 1\n",
    "for X_train, X_val, X_test, y_train, y_val, y_test in utils.kfold(X, y, num_fold=kfold):\n",
    "    print('\\n%d fold start..............\\n' % (fold))\n",
    "    #\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max_scaler.fit(X_train)\n",
    "\n",
    "    X_train = min_max_scaler.transform(X_train)\n",
    "    X_val = min_max_scaler.transform(X_val)\n",
    "    X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "    X_train_conv = utils.process_data_for_conv2D(X_train)\n",
    "    X_val_conv = utils.process_data_for_conv2D(X_val)\n",
    "    X_test_conv = utils.process_data_for_conv2D(X_test)\n",
    "    \n",
    "    train_data_iter = utils.data_iter(X_train_conv, y_train, batch_size)\n",
    "    \n",
    "    #\n",
    "    model = cnn_model(input_shape=(1, 285, 1))\n",
    "    model.compile(\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                optimizer=opt)\n",
    "#     model.summary()\n",
    "    \n",
    "    #\n",
    "    best_train_auc = 0\n",
    "    best_train_ks = 0\n",
    "    best_val_auc = 0\n",
    "    best_val_ks = 0\n",
    "    best_test_auc = 0\n",
    "    best_test_ks = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        batchs = 0\n",
    "        for X_batch, y_batch in train_data_iter:\n",
    "            model.train_on_batch(X_batch, y_batch, class_weight=class_weight)\n",
    "            batchs += 1\n",
    "            if batchs >= len(X_train) / batch_size:\n",
    "                break\n",
    "        \n",
    "        train_loss, train_acc = model.evaluate(X_train_conv, y_train, verbose=0)\n",
    "        \n",
    "        y_train_pre = model.predict(X_train_conv)\n",
    "        y_val_pre = model.predict(X_val_conv)\n",
    "        y_test_pre = model.predict(X_test_conv)\n",
    "        \n",
    "        train_auc = utils.cal_auc(y_train, y_train_pre)\n",
    "        val_auc = utils.cal_auc(y_val, y_val_pre)\n",
    "        test_auc = utils.cal_auc(y_test, y_test_pre)\n",
    "\n",
    "        train_ks = utils.get_ks_score(y_train, y_train_pre)\n",
    "        val_ks = utils.get_ks_score(y_val, y_val_pre)\n",
    "        test_ks = utils.get_ks_score(y_test, y_test_pre)\n",
    "        \n",
    "        train_performance = utils.get_model_key_performance(y_train, y_train_pre)\n",
    "        val_performance = utils.get_model_key_performance(y_val, y_val_pre)\n",
    "        test_performance = utils.get_model_key_performance(y_test, y_test_pre)\n",
    "        if val_ks > best_val_ks:\n",
    "            model.save('best_model_' + str(fold) + '.hdf5')\n",
    "            best_train_auc = train_auc\n",
    "            best_train_ks = train_ks\n",
    "            best_val_auc = val_auc\n",
    "            best_val_ks = val_ks\n",
    "            best_test_auc = test_auc\n",
    "            best_test_ks = test_ks\n",
    "            \n",
    "        print('Epoch %d train_loss %.4f train_acc %.4f, train_auc %.4f train_ks %.4f, val_auc %.4f val_ks %.4f, test_auc %.4f test_ks %.4f' % \n",
    "              (e+1, train_loss, train_acc, train_auc, train_ks, val_auc, val_ks, test_auc, test_ks))\n",
    "        print('Eopch %d train_performance %.4f val_performance %.4f test_performance %.4f' % \n",
    "              (train_performance, val_performance, test_performance))\n",
    "    fold = fold + 1\n",
    "    \n",
    "    \n",
    "    best_train_auc_record.append(best_train_auc)\n",
    "    best_train_ks_record.append(best_train_ks)\n",
    "    best_val_auc_record.append(best_val_auc)\n",
    "    best_val_ks_record.append(best_val_ks)\n",
    "    best_test_auc_record.append(best_test_auc)\n",
    "    best_test_ks_record.append(best_test_ks)\n",
    "\n",
    "best_train_auc_avg = np.array(best_train_auc_record).mean()\n",
    "best_train_ks_avg = np.array(best_train_ks_record).mean()\n",
    "best_val_auc_avg = np.array(best_val_auc_record).mean()\n",
    "best_val_ks_avg = np.array(best_val_ks_record).mean()\n",
    "best_test_auc_avg = np.array(best_test_auc_record).mean()\n",
    "best_test_ks_avg = np.array(best_test_ks_record).mean()\n",
    "print('\\n\\n%d fold train_auc_avg %.4f train_ks_avg %.4f, val_auc_avg %.4f val_ks_avg %.4f, test_auc_avg %.4f test_ks_avg %.4f' %\n",
    "    (kfold, best_train_auc_avg, best_train_ks_avg, best_val_auc_avg, best_val_ks_avg, best_test_auc_avg, best_test_ks_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48606601731601734,\n",
       " 0.571900123685838,\n",
       " 0.47901205936920227,\n",
       " 0.3877203153988868,\n",
       " 0.38022959183673466,\n",
       " 0.563006338899196,\n",
       " 0.4800711193568336,\n",
       " 0.5027829313543599,\n",
       " 0.4773732220160791,\n",
       " 0.4956516697588126]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val_ks_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40354477611940304,\n",
       " 0.4559255007016201,\n",
       " 0.41153208317387424,\n",
       " 0.4621299098911039,\n",
       " 0.41234967354370333,\n",
       " 0.41531851233343775,\n",
       " 0.3917881455194888,\n",
       " 0.45880156327917526,\n",
       " 0.44102912013359774,\n",
       " 0.43589743589743596]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_test_ks_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
